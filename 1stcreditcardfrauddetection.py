# -*- coding: utf-8 -*-
"""1stcreditcardFraudDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuu3tCZjEBWBDsDyRIIflju3LL3uOwuL

#Problem Statement: Credit Card Fraud Detection Using Machine Learning and Deep Learning
The prevalence of credit card fraud continues to pose significant financial and security risks for individuals, businesses, and financial institutions. Traditional rule-based systems are often insufficient in detecting increasingly sophisticated fraudulent activities. To address this challenge, the aim of this study is to develop and evaluate machine learning and deep learning models for accurate and efficient credit card fraud detection.
#Background:
The dataset used in this study contains credit card transactions made by European cardholders in September 2013. It includes features resulting from Principal Component Analysis (PCA) transformation, which ensures the confidentiality of the original data. The dataset is highly imbalanced, with a very small proportion of fraudulent transactions compared to legitimate ones.
1.	Data Source and Timeframe: The dataset consists of credit card transactions that occurred in September 2013. The transactions were made by European cardholders.
2.	Transaction Details: The dataset covers transactions over a span of two days. Out of the 284,807 transactions in the dataset, 492 of them are labelled as fraudulent (positive class). This means that the vast majority (99.828%) of the transactions are legitimate (negative class).
3.	Data Imbalance: The dataset is highly imbalanced due to the small number of fraudulent transactions compared to legitimate ones. This imbalance is common in real-world fraud detection scenarios, as fraudulent transactions are typically a rare occurrence.
4.	Features: The dataset contains only numerical input features. Most of these features (V1 to V28) are the result of applying Principal Component Analysis (PCA) transformation to the original features. Unfortunately, the original features and additional background information are not provided due to confidentiality issues. The 'Time' feature represents the time elapsed between a transaction and the first transaction in the dataset. The 'Amount' feature represents the transaction amount.
5.	Response Variable: The 'Class' feature is the response variable that indicates whether a transaction is fraudulent (1) or legitimate (0).

#Objective:
 The primary objective of this study is to build and evaluate predictive models capable of identifying fraudulent credit card transactions with high accuracy, while minimizing false positives and false negatives. This involves the application of various machine learning and deep learning techniques to effectively handle the imbalanced nature of the dataset and capture intricate patterns associated with fraudulent activities.
#Approach:

1.	Data Exploration, Comprehension, and Visualization
2.	Data Preprocessing for Model Development
3.	Constructing the Model
4.	Assessing Model Performance

#Import Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, ShuffleSplit, learning_curve
from sklearn.preprocessing import StandardScaler

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM
from sklearn.model_selection import learning_curve

"""#Data collection"""

df = pd.read_csv('creditcard.csv')
df.head()

print(df.isnull().sum().max())
df.info()

#let us look at the number of unique values in the dataset
df.nunique()

# Summary statistics of numerical features
numerical_summary = df.describe()
print(numerical_summary)

"""#Data exploration"""

#determing the shape of the dataset
df.shape

amount_mean = df['Amount'].mean().round(2)
print("Amount of money on average per transactions:" , amount_mean, )

"""From this information, it's evident that the dataset exhibits an imbalance, with a higher proportion of non-fraudulent transactions compared to fraudulent ones. The dataset also provides an average transaction amount of 88.35. Given this class imbalance, it becomes crucial to utilize data visualization techniques during the data preparation phase to visually assess the extent of this imbalance."""

df.columns

# Ratio of Frauds and Non- Frauds data
print('Non-Frauds', round(df['Class'].value_counts()[0]/len(df)* 100,2),"% of the dataset")
print('Frauds', round(df['Class'].value_counts()[1]/len(df)* 100,2),"% of the dataset")

"""the code calculates and displays the percentage distribution of fraudulent and non-fraudulent transactions in the dataset:

Non-Frauds: Approximately 99.83% of the dataset consists of non-fraudulent transactions.
Frauds: About 0.17% of the dataset corresponds to fraudulent transactions.
"""

ax = sns.countplot(x=df['Class'])
plt.title("Ratio per Class \n [0 = Non-Fraud || 1 = Fruad]")
for container in ax.containers:
    ax.bar_label(container)
plt.show()

fig, ax = plt.subplots(1, 2, figsize=(18, 4))

amount_val = df['Amount'].values
time_val = df['Time'].values

sns.distplot(amount_val, ax=ax[0],kde=True)
ax[0].set_title('Distribution of Transaction Amount (Fraud Dataset)', fontsize=14)
ax[0].set_xlabel('Amount')
ax[0].set_ylabel('Count')
ax[0].set_xlim([min(amount_val), max(amount_val)])

sns.distplot(time_val, ax=ax[1],kde=True)
ax[1].set_title('Distribution of Transaction Time (Fraud Dataset)', fontsize=14)
ax[1].set_xlabel('Time')
ax[1].set_ylabel('Count')
ax[1].set_xlim([min(time_val), max(time_val)])
plt.show()

"""By observing these distribution plots, we can gain insights into the dataset's characteristics:

•	The transaction amount distribution reveals how frequently different transaction amounts occur, helping to understand common and rare transaction values.

•	The transaction time distribution showcases when transactions were made, providing insights into time patterns and potential trends.

"""

import seaborn as sns
import matplotlib.pyplot as plt

# Create subplots with specified size
fig, plots = plt.subplots(figsize=(15, 10))

# Plotting the 'Amount' feature
# Box plot
plots = sns.boxplot(df['Amount'], ax=plots)

# Distribution plots
sns.histplot(data=df[df['Amount'] <= 1500], x='Amount', bins=50, ax=plots)
sns.histplot(data=df[(df['Class'] == 0) & (df['Amount'] <= 1500)], x='Amount', bins=50, ax=plots)
sns.histplot(data=df[(df['Class'] == 1) & (df['Amount'] <= 1500)], x='Amount', bins=50, ax=plots)

# Setting titles
plots.set_title('Amount Distribution')

# Setting x and y labels
plots.set_xlabel('Transaction Amount')
plots.set_ylabel('Number of Transactions')

# Adding legends for distribution types
plots.legend(labels=['Boxplot', 'Overall', 'Non Fraud', 'Fraud'])

plt.show()

"""This code generates a comprehensive visualization that provides insights into the distribution of transaction amounts in the dataset. By comparing the overall distribution with the distributions of non-fraudulent and fraudulent transactions, viewers can gain a deeper understanding of the transaction amount patterns. The box plot highlights central tendencies and potential outliers, while the histograms reveal the frequency of transactions within different amount ranges. This visualization is particularly useful for identifying any significant differences in transaction amount patterns between legitimate and fraudulent transactions.





"""

# Correlation matrix heatmap
correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False)
plt.title("Correlation Matrix Heatmap")
plt.show()

"""This code creates a heatmap that visually represents the correlation between numerical features in the dataset. The colour intensity of each cell in the heatmap indicates the strength and direction of the correlation between two features. Positive correlations are depicted using warm colours (such as red), while negative correlations are depicted using cool colours (such as blue). A neutral colour (usually white or a pale shade) represents low or no correlation.

By examining this heatmap, we can quickly identify which features are strongly positively or negatively correlated with each other. This can be valuable for identifying potential multicollinearity between features, understanding relationships between variables, and selecting features for modelling.

"""

# Pairplot of selected features
selected_features = ['Amount', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8']
sns.pairplot(df[selected_features], diag_kind='kde')
plt.show()

"""This code creates a matrix of scatterplots where each scatterplot shows the relationship between two selected numerical features. Along the diagonal, instead of scatterplots, KDE plots are shown to visualize the distribution of individual features. The purpose of this visualization is to understand potential patterns, correlations, and distributions between the selected features. Scatterplots help identify linear or nonlinear relationships, while KDE plots show how the data is distributed along each feature.

By analyzing this pairplot, we can quickly identify visual patterns that might indicate correlations or trends between pairs of features. This can be useful for understanding feature relationships, detecting potential outliers, and deciding which features might be relevant for building machine learning models.

"""

# Check for missing values
missing_values = df.isnull().sum()
print("Missing Values:\n", missing_values)

# Pair plot for selected features
sns.pairplot(df[['Time', 'Amount', 'Class']], hue='Class', plot_kws={'alpha': 0.5})
plt.title("Pair Plot of Time, Amount, and Class")
plt.show()

"""It is a  matrix of scatterplots where each scatterplot shows the relationship between pairs of features 'Time' and 'Amount'. The color of the points in the scatterplots is determined by the 'Class' feature, with fraudulent transactions (Class 1) and non-fraudulent transactions (Class 0) differentiated by color. This visualization helps understand the distribution and relationships between time, amount, and the class of transactions. It can provide insights into whether certain time periods or transaction amounts are associated with a higher likelihood of fraud."""

# Select numerical features
numerical_features = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',
                      'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',
                      'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27',
                      'V28', 'Amount']

# Box plots to visualize outliers
plt.figure(figsize=(14, 10))
for feature in numerical_features:
    plt.subplot(5, 6, numerical_features.index(feature) + 1)
    sns.boxplot(x='Class', y=feature, data=df)
    plt.title(f'Boxplot of {feature}')
plt.tight_layout()
plt.show()

# Z-score method to detect outliers
from scipy import stats
z_scores = np.abs(stats.zscore(df[numerical_features]))
outliers = (z_scores > 3)
outlier_percentage = np.sum(outliers) / (df.shape[0] * df.shape[1]) * 100
print("Percentage of outliers:", outlier_percentage)

"""This analysis helps in understanding whether certain features exhibit unusual values in relation to the rest of the data and whether those outliers differ between fraudulent and non-fraudulent transactions.

#Data processing
"""

# Separate features and target variable
X = df.drop(columns=['Class'])
y = df['Class']

"""The original dataset is separated it into two parts: the feature matrix (X) and the target variable (y). The feature matrix includes all the features except for the target variable, while the target variable includes the values of the 'Class' column, which indicates whether a transaction is fraudulent (Class 1) or non-fraudulent (Class 0). This separation is a common preprocessing step in machine learning to prepare the data for training models, as it ensures that the input features and the output labels are organized appropriately.







"""

# Preprocessing: Scaling numerical features and train-test split
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""It nvolves two essential preprocessing steps:
1.	Feature Scaling:
The numerical features are standardized using the StandardScaler to ensure that they have similar scales. This is important for many machine learning algorithms that are sensitive to the scale of input features.
2.	Train-Test Split:
The dataset is split into training and testing subsets, with 80% used for training and 20% for testing. This separation is crucial to evaluate the model's performance on unseen data and prevent overfitting.

In summary, this code prepares the data for machine learning by scaling the features and splitting the data into training and testing sets, which are essential steps in building and evaluating predictive models.

"""

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

"""The issue of class imbalance in the training data is presented by using the SMOTE technique.

**SMOTE Resampling:**

The SMOTE technique generates synthetic samples for the minority class (fraudulent transactions) to create a balanced class distribution. This helps to prevent the model from being biased towards the majority class and improves its ability to generalize well to both classes.

In summary, by applying SMOTE, this code aims to create a more balanced training dataset, which can lead to improved model performance when dealing with imbalanced classes.

#Implementation and Evaluation of Machine learning Models

This section demonstrates the process of building and evaluating several machine learning models for credit card fraud detection using the provided dataset. It covers the Random Forest, Decision Tree, Logistic Regression, and K-Nearest Neighbors models. Let's break down the structure and the process of building these models:
1.	Model Initialization:
For each model (Random Forest, Decision Tree, Logistic Regression, and K-Nearest Neighbors), the code initializes a corresponding model object with specified hyperparameters.
2.	Training the Models:
For each model, the resampled training data (X_train_resampled, y_train_resampled) obtained from applying SMOTE is used to train the model using the fit method.
3.	Making Predictions:
After training each model, predictions are made on the test set (X_test) using the predict method, and the predicted labels are stored in y_rf_pred, y_dt_pred, y_lr_pred, and y_knn_pred.
4.	Evaluating the Models:For each model, various evaluation metrics are computed and printed:

•	Accuracy: Calculated using accuracy_score by comparing the predicted labels (y_rf_pred, y_dt_pred, etc.) with the actual test labels (y_test).

•	Confusion Matrix: Generated using confusion_matrix to show the counts of true positive, true negative, false positive, and false negative predictions.

•	Classification Report: Generated using classification_report to display metrics such as precision, recall, F1-score, and support for both classes.

5.	Model Comparison:
•	The printed evaluation metrics (accuracy, confusion matrix, classification report) for each model provide a comparison of their performance on the test set.

Model Building Process:
1.	The process starts by initializing each model with specific hyperparameters.
2.	The models are trained using the resampled training data created through SMOTE, which addresses class imbalance.
3.	Predictions are made on the test set for each model.
4.	Model performance is evaluated using accuracy, confusion matrix, and classification report metrics.
5.	The results for all models are compared to understand which one performs best for the task of credit card fraud detection.
"""

# Random Forest model with simplified hyperparameters
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Train the Random Forest model
rf_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_rf_pred = rf_model.predict(X_test)

# Evaluate the Random Forest model
rf_accuracy = accuracy_score(y_test, y_rf_pred)
rf_conf_matrix = confusion_matrix(y_test, y_rf_pred)
rf_class_report = classification_report(y_test, y_rf_pred)

print("Random Forest Accuracy:", rf_accuracy)
print("Random Forest Confusion Matrix:\n", rf_conf_matrix)
print("Random Forest Classification Report:\n", rf_class_report)

# Decision Tree model
dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)

# Train the Decision Tree model
dt_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_dt_pred = dt_model.predict(X_test)

# Evaluate the Decision Tree model
dt_accuracy = accuracy_score(y_test, y_dt_pred)
dt_conf_matrix = confusion_matrix(y_test, y_dt_pred)
dt_class_report = classification_report(y_test, y_dt_pred)

print("Decision Tree Accuracy:", dt_accuracy)
print("Decision Tree Confusion Matrix:\n", dt_conf_matrix)
print("Decision Tree Classification Report:\n", dt_class_report)

# Logistic Regression model
lr_model = LogisticRegression(random_state=42)

# Train the Logistic Regression model
lr_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_lr_pred = lr_model.predict(X_test)

# Evaluate the Logistic Regression model
lr_accuracy = accuracy_score(y_test, y_lr_pred)
lr_conf_matrix = confusion_matrix(y_test, y_lr_pred)
lr_class_report = classification_report(y_test, y_lr_pred)

print("Logistic Regression Accuracy:", lr_accuracy)
print("Logistic Regression Confusion Matrix:\n", lr_conf_matrix)
print("Logistic Regression Classification Report:\n", lr_class_report)

# K-Nearest Neighbors model
knn_model = KNeighborsClassifier(n_neighbors=5)

# Train the K-Nearest Neighbors model
knn_model.fit(X_train_resampled, y_train_resampled)

# Make predictions on the test set
y_knn_pred = knn_model.predict(X_test)

# Evaluate the K-Nearest Neighbors model
knn_accuracy = accuracy_score(y_test, y_knn_pred)
knn_conf_matrix = confusion_matrix(y_test, y_knn_pred)
knn_class_report = classification_report(y_test, y_knn_pred)

print("K-Nearest Neighbors Accuracy:", knn_accuracy)
print("K-Nearest Neighbors Confusion Matrix:\n", knn_conf_matrix)
print("K-Nearest Neighbors Classification Report:\n", knn_class_report)

"""#Result Evaluation
 Describing and explaining the predicted results of the models based on the provided evaluation metrics:

**Random Forest Model:**

•	Accuracy: 99.81%

•	Precision (fraudulent transactions): 48%

•	Recall (fraudulent transactions): 89%

•	F1-score (fraudulent transactions): 62%

Explanation: The Random Forest model achieves high accuracy and is able to detect a significant portion of fraudulent transactions (high recall). However, the precision is relatively low, indicating that some of the predicted fraudulent transactions are false positives. This trade-off between precision and recall is reflected in the F1-score, which balances both metrics. Overall, the model performs well in detecting fraudulent transactions, but there is room for improvement in reducing false positives.

**Decision Tree Model:**

•	Accuracy: 98.43%

•	Precision (fraudulent transactions): 9%

•	Recall (fraudulent transactions): 84%

•	F1-score (fraudulent transactions): 15%

Explanation: The Decision Tree model achieves a high accuracy, but the precision is very low, indicating a high rate of false positives. The recall is relatively high, showing that the model detects a significant portion of fraudulent transactions, but the trade-off between precision and recall results in a low F1-score. This suggests that the model is missing some fraudulent transactions while producing many false positives.

**Logistic Regression Model:**
•	Accuracy: 97.46%

•	Precision (fraudulent transactions): 6%

•	Recall (fraudulent transactions): 92%

•	F1-score (fraudulent transactions): 11%

Explanation: The Logistic Regression model achieves good accuracy and high recall, indicating it's able to capture a large portion of fraudulent transactions. However, the precision is very low, meaning there are many false positives. The F1-score is also low due to the imbalance between precision and recall. This model shows promise in identifying fraudulent transactions but needs improvement in reducing false positives.

**K-Nearest Neighbors Model:**

•	Accuracy: 99.82%

•	Precision (fraudulent transactions): 48%

•	Recall (fraudulent transactions): 87%

•	F1-score (fraudulent transactions): 62%

Explanation: The K-Nearest Neighbors model achieves high accuracy and is effective at detecting fraudulent transactions, with a balance between precision and recall. Similar to the Random Forest model, there's a trade-off between precision and recall, as indicated by the F1-score. The model performs well overall, but there's still room for improvement in terms of reducing false positives.

**Summary:** All models achieve high accuracy, but there's a common trade-off between precision and recall, leading to moderate F1-scores. This suggests that the models have the potential to detect a significant portion of fraudulent transactions, but they also produce false positives. Further model tuning and potentially exploring ensemble methods or more sophisticated algorithms might help in achieving a better balance between precision and recall, ultimately leading to improved performance in credit card fraud detection.

#About learning curve
Learning curves are visual representations that show how the performance of a machine learning model changes as the amount of training data increases. In the provided code, learning curves are generated for different models (Random Forest, Decision Tree, Logistic Regression, and K-Nearest Neighbors) using the learning_curve function from sklearn.model_selection.

The learning curves display the following:

**Training score** : This curve represents how well the model performs on the training data as the number of training examples increases. It provides insight into the model's ability to fit the training data.

**Cross-validation score**: This curve illustrates the model's performance on cross-validation data (not seen during training) as the training dataset size grows. It gives an indication of the model's generalization ability.

The shaded regions around the curves indicate the variability (standard deviation) of the scores at each training dataset size.

**Summary:**

As the training dataset size increases, the training score tends to decrease, while the cross-validation score tends to increase.
Initially, with a small training dataset, the training score is high as the model overfits the data (low bias, high variance).
As more data is added, the training score decreases, and the cross-validation score increases, indicating the model is finding a better balance between bias and variance.

Eventually, the two curves tend to stabilize, suggesting that additional training data might not significantly improve the model's performance.
In the provided learning curves:

The Random Forest and K-Nearest Neighbors models seem to generalize well as the dataset size increases, as indicated by the convergence of the training and cross-validation scores.

The Decision Tree model shows a larger gap between the two curves, which might indicate overfitting.

The Logistic Regression model also demonstrates a gap between the curves, suggesting potential room for improvement with more data or model tuning.

Overall, learning curves provide valuable insights into a model's bias-variance trade-off and can help guide decisions about dataset size, model complexity, and further optimization.
"""

from sklearn.model_selection import learning_curve

# Define a function to plot learning curves
def plot_learning_curve(estimator, X, y, title):
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.figure()
    plt.title(title)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")

    plt.legend(loc="best")
    return plt

# Plot learning curve for Random Forest model
plot_learning_curve(rf_model, X_train_resampled, y_train_resampled, title="Random Forest Learning Curve").show()

# Plot learning curve for Decision Tree model
plot_learning_curve(dt_model, X_train_resampled, y_train_resampled, title="Decision Tree Learning Curve").show()

# Plot learning curve for Logistic Regression model
plot_learning_curve(lr_model, X_train_resampled, y_train_resampled, title="Logistic Regression Learning Curve").show()

# Plot learning curve for K-Nearest Neighbors model
plot_learning_curve(knn_model, X_train_resampled, y_train_resampled, title="K-Nearest Neighbors Learning Curve").show()

"""# Implementation and Evaluation of Machine learning Models

Discussing the structure of the CNN and RNN models and how they were constructed:

**Convolutional Neural Network (CNN):**
The CNN is a type of deep learning model well-suited for processing grid-like data such as images and sequences. In your code, you've built a simple 1D CNN for fraud detection:

Model Structure:

The CNN model consists of multiple layers:

Conv1D layer: This layer applies convolutional filters to the input data. It has 64 filters with a kernel size of 3 and uses the ReLU activation function.

MaxPooling1D layer: This layer performs max pooling to reduce the spatial dimensions of the data.

Flatten layer: This layer converts the pooled feature maps into a 1D vector.

Dense layers: There are two dense layers with 128 and 1 neurons
respectively. The first dense layer uses the ReLU activation function, while the last dense layer uses the sigmoid activation function for binary classification.

Compilation:

The model is compiled using the Adam optimizer and binary cross-entropy loss, which is suitable for binary classification tasks. The accuracy metric is also specified for evaluation.
Training:

The model is trained using the training data (X_train_resampled and y_train_resampled) reshaped to fit the input shape of the model. The model is trained for 10 epochs with a batch size of 64.

**Evaluation:**

After training, the model's predictions are obtained for the test data (X_test) and then thresholded using 0.5 to convert them into binary predictions. The accuracy, confusion matrix, and classification report are printed to evaluate the model's performance.
Recurrent Neural Network (RNN):
RNNs are well-suited for sequential data, where the order of elements matters. Here's how you've constructed the RNN:

**Model Structure:**

The RNN model is built using the Long Short-Term Memory (LSTM) cell, a type of recurrent unit that can capture long-range dependencies in sequences.

LSTM layer: This layer has 64 LSTM units with the ReLU activation function and takes the input shape of the resampled training data.
Compilation and Training:

Similar to the CNN, the RNN is compiled using the Adam optimizer and binary cross-entropy loss. The model is trained using the reshaped training data for 10 epochs with a batch size of 64.

**Evaluation:**

After training, the model's predictions are obtained for the test data and thresholded using 0.5 for binary predictions. The accuracy, confusion matrix, and classification report are printed to evaluate the model's performance.
Building the Models:

Both models are constructed using the Keras Sequential API, allowing you to stack layers sequentially.
The choice of layers, activation functions, and optimizer is based on experimentation, known best practices, and the nature of the data (sequential for RNN).

**Summary:**
Both a CNN and an RNN were built and trained  for credit card fraud detection. CNNs are effective for identifying patterns in sequences, while RNNs are suitable for sequential data. The performance of these models can vary based on their architecture, hyperparameters, and the characteristics of the dataset. Experimentation and tuning may be necessary to optimize their performance further.
"""

# CNN model
cnn_model = Sequential()
cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_resampled.shape[1], 1)))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='relu'))
cnn_model.add(Dense(1, activation='sigmoid'))

cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the CNN model
cnn_history = cnn_model.fit(X_train_resampled.reshape(X_train_resampled.shape[0], X_train_resampled.shape[1], 1), y_train_resampled, epochs=10, batch_size=64)

# Evaluate the CNN model
cnn_pred = cnn_model.predict(X_test.reshape(X_test.shape[0], X_test.shape[1], 1))
cnn_pred = (cnn_pred > 0.5)
cnn_accuracy = accuracy_score(y_test, cnn_pred)
cnn_conf_matrix = confusion_matrix(y_test, cnn_pred)
cnn_class_report = classification_report(y_test, cnn_pred)

print("CNN Accuracy:", cnn_accuracy)
print("CNN Confusion Matrix:\n", cnn_conf_matrix)
print("CNN Classification Report:\n", cnn_class_report)

# RNN model
rnn_model = Sequential()
rnn_model.add(LSTM(64, activation='relu', input_shape=(X_train_resampled.shape[1], 1)))
rnn_model.add(Dense(1, activation='sigmoid'))

rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the RNN model
rnn_history = rnn_model.fit(X_train_resampled.reshape(X_train_resampled.shape[0], X_train_resampled.shape[1], 1), y_train_resampled, epochs=10, batch_size=64)

# Evaluate the RNN model
rnn_pred = rnn_model.predict(X_test.reshape(X_test.shape[0], X_test.shape[1], 1))
rnn_pred = (rnn_pred > 0.5)
rnn_accuracy = accuracy_score(y_test, rnn_pred)
rnn_conf_matrix = confusion_matrix(y_test, rnn_pred)
rnn_class_report = classification_report(y_test, rnn_pred)

print("RNN Accuracy:", rnn_accuracy)
print("RNN Confusion Matrix:\n", rnn_conf_matrix)
print("RNN Classification Report:\n", rnn_class_report)

"""#Result Evaluation
**CNN Model:**

The CNN model achieved an accuracy of approximately 99.90%.

The confusion matrix indicates that out of 56,864 non-fraudulent transactions, 56,825 were correctly predicted as non-fraudulent (true negatives). Additionally, out of 98 fraudulent transactions, 82 were correctly predicted as fraudulent (true positives).

The classification report provides further insights into the precision, recall, and F1-score. The model achieved a precision of 0.68 for detecting fraudulent transactions, indicating that when it predicts a transaction as fraudulent, it's correct 68% of the time. The recall (true positive rate) is 0.84, which suggests that the model is able to identify 84% of the actual fraudulent transactions.

**RNN Model:**

The RNN model achieved an accuracy of approximately 99.53%.

The confusion matrix indicates that out of 56,864 non-fraudulent transactions, 56,609 were correctly predicted as non-fraudulent (true negatives). Moreover, out of 98 fraudulent transactions, 86 were correctly predicted as fraudulent (true positives).

The classification report highlights that the model has a lower precision of 0.25 for detecting fraudulent transactions, indicating that its predictions for fraud may have a higher false positive rate. However, the recall is 0.88, suggesting that the model is able to identify 88% of the actual fraudulent transactions.

**Summary:**

The CNN model outperforms the RNN model in terms of accuracy, precision, and recall for fraud detection.

The CNN model has a higher precision and recall, indicating that it's better at correctly identifying both non-fraudulent and fraudulent transactions.

The RNN model has a lower precision and higher recall, implying that it may produce more false positives but captures a larger proportion of actual fraud cases.

Overall, based on the provided results, the CNN model appears to be the better performer between the two for this specific fraud detection task.





"""

# Train the CNN model
cnn_history = cnn_model.fit(X_train_resampled.reshape(X_train_resampled.shape[0], X_train_resampled.shape[1], 1), y_train_resampled, epochs=10, batch_size=64, validation_data=(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test))

# Train the RNN model
rnn_history = rnn_model.fit(X_train_resampled.reshape(X_train_resampled.shape[0], X_train_resampled.shape[1], 1), y_train_resampled, epochs=10, batch_size=64, validation_data=(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), y_test))

# Plot learning curves for CNN and RNN
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(cnn_history.history['accuracy'], label='CNN Training Accuracy')
plt.plot(cnn_history.history['val_accuracy'], label='CNN Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('CNN Learning Curve')

plt.subplot(1, 2, 2)
plt.plot(rnn_history.history['accuracy'], label='RNN Training Accuracy')
plt.plot(rnn_history.history['val_accuracy'], label='RNN Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('RNN Learning Curve')

plt.tight_layout()
plt.show()

"""Both models show a common trend where training accuracy increases with epochs.

For both models, validation accuracy tends to increase initially but might plateau or decrease slightly later. This suggests potential overfitting, where the models start to perform well on the training data but struggle to generalize to new, unseen data (validation set).

In terms of learning curves, the CNN model and the RNN model exhibit similar behaviors, with the CNN model slightly outperforming the RNN model in terms of validation accuracy.

It's important to monitor learning curves to strike the right balance between model complexity and generalization, and to consider techniques like regularization to mitigate overfitting if necessary.

# Comparative Analysis
Among the models we have trained, the CNN (Convolutional Neural Network) model appears to provide the best results for credit card fraud detection. Let's explain why the CNN model performed the best in detail:

**Model Performances and Explanation:**

1. **Random Forest:**
   - Accuracy: 99.81%
   - F1-score (Fraud Class): 0.62
   - Random Forest achieved high accuracy but has a relatively lower F1-score for the fraud class. It might be performing well on non-fraudulent cases but is not as effective in identifying fraud cases.

2. **Decision Tree:**
   - Accuracy: 98.43%
   - F1-score (Fraud Class): 0.15
   - The Decision Tree model has good accuracy, but its low F1-score for the fraud class indicates it might have a higher number of false negatives, classifying some fraud cases as non-fraud.

3. **Logistic Regression:**
   - Accuracy: 97.46%
   - F1-score (Fraud Class): 0.11
   - While Logistic Regression has high accuracy, its low F1-score for the fraud class suggests it struggles to effectively detect fraud cases, leading to a high number of false negatives.

4. **K-Nearest Neighbors:**
   - Accuracy: 99.82%
   - F1-score (Fraud Class): 0.62
   - K-Nearest Neighbors achieved high accuracy but has an F1-score similar to the Random Forest model. It might have challenges in effectively capturing complex patterns associated with fraud cases.

5. **CNN:**
   - Accuracy: 99.90%
   - F1-score (Fraud Class): 0.75
   - The CNN model achieved the highest accuracy and F1-score for the fraud class. It excels at capturing complex patterns and relationships in sequential data, making it particularly suitable for fraud detection where intricate patterns are crucial.

6. **RNN:**
   - Accuracy: 99.53%
   - F1-score (Fraud Class): 0.39
   - The RNN model has high accuracy, but its F1-score for the fraud class is lower compared to the CNN model. This suggests that while it's identifying fraud cases, it might also be generating a higher number of false positives.

**Why the CNN Model Performed Best:**

- **Feature Extraction:** CNNs are effective at learning relevant features from sequential data. In the case of credit card transactions, CNNs can capture hidden patterns that other models might miss.
- **Pattern Recognition:** Fraudulent activities often involve intricate patterns that can be better captured by CNNs' ability to identify local patterns and hierarchies.
- **Complex Relationships:** CNNs can identify relationships between different features and their impact on the prediction, which is important for capturing sophisticated fraud patterns.

**Conclusion:**

Considering the high accuracy, F1-score, and the ability to capture complex patterns, the CNN model stands out as the best performer for credit card fraud detection. Its superior performance in correctly classifying fraud cases while maintaining a good balance between precision and recall makes it the recommended choice for this specific task.
"""

